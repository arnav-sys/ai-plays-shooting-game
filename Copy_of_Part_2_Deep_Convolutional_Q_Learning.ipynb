{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Part 2 - Deep Convolutional Q-Learning",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnav-sys/ai-plays-shooting-game/blob/main/Copy_of_Part_2_Deep_Convolutional_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaC5dBzI1pFK"
      },
      "source": [
        "# Part 2 - Deep Convolutional Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQVnB6cvvZ4A"
      },
      "source": [
        "### Installing system dependencies for VizDoom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkVnx4F0s3dD"
      },
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev libopenal-dev timidity libwildmidi-dev unzip\n",
        "!sudo apt-get install libboost-all-dev\n",
        "!apt-get install liblua5.1-dev\n",
        "!sudo apt-get install cmake libboost-all-dev libgtk2.0-dev libsdl2-dev python-numpy git\n",
        "!git clone https://github.com/shakenes/vizdoomgym.git\n",
        "!python3 -m pip install -e vizdoomgym/\n",
        "!pip install pillow\n",
        "!pip install scipy==1.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYSYxa3A3rPm"
      },
      "source": [
        "### **IMPORTANT NOTE: After installing all dependencies, restart your runtime**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqwU0xvmps6f"
      },
      "source": [
        "## ------------------------- IMAGE PREPROCESSING (image_preprocessing.py) -------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNxep0W2v5eR"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av98EDA6wAyb"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.misc import imresize\n",
        "from gym.core import ObservationWrapper\n",
        "from gym.spaces.box import Box"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYg83uMKwW9j"
      },
      "source": [
        "### Preprocessing the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAKQuJpa3EEI"
      },
      "source": [
        "class PreprocessImage(ObservationWrapper):\n",
        "    \n",
        "    def __init__(self, env, height = 64, width = 64, grayscale = True, crop = lambda img: img):\n",
        "        super(PreprocessImage, self).__init__(env)\n",
        "        self.img_size = (height, width)\n",
        "        self.grayscale = grayscale\n",
        "        self.crop = crop\n",
        "        n_colors = 1 if self.grayscale else 3\n",
        "        self.observation_space = Box(0.0, 1.0, [n_colors, height, width])\n",
        "\n",
        "    def observation(self, img):\n",
        "        img = self.crop(img)\n",
        "        img = imresize(img, self.img_size)\n",
        "        if self.grayscale:\n",
        "            img = img.mean(-1, keepdims = True)\n",
        "        img = np.transpose(img, (2, 0, 1))\n",
        "        img = img.astype('float32') / 255.\n",
        "        return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkb3sY94oxR0"
      },
      "source": [
        "## ------------------------- EXPERIENCE REPLAY (experience_replay.py) -------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m77JIgY5_3-i"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyftpA_J_69n"
      },
      "source": [
        "import numpy as np\n",
        "from collections import namedtuple, deque"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMaHr_X-oiXJ"
      },
      "source": [
        "### Defining One Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQZwk7xgomh_"
      },
      "source": [
        "Step = namedtuple('Step', ['state', 'action', 'reward', 'done'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeA93OwL97Mn"
      },
      "source": [
        "### Making the AI progress on several (n_step) steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQDLILP-x6h3"
      },
      "source": [
        "class NStepProgress:\n",
        "    \n",
        "    def __init__(self, env, ai, n_step):\n",
        "        self.ai = ai\n",
        "        self.rewards = []\n",
        "        self.env = env\n",
        "        self.n_step = n_step\n",
        "    \n",
        "    def __iter__(self):\n",
        "        state = self.env.reset()\n",
        "        history = deque()\n",
        "        reward = 0.0\n",
        "        while True:\n",
        "            action = self.ai(np.array([state]))[0][0]\n",
        "            next_state, r, is_done, _ = self.env.step(action)\n",
        "            reward += r\n",
        "            history.append(Step(state = state, action = action, reward = r, done = is_done))\n",
        "            while len(history) > self.n_step + 1:\n",
        "                history.popleft()\n",
        "            if len(history) == self.n_step + 1:\n",
        "                yield tuple(history)\n",
        "            state = next_state\n",
        "            if is_done:\n",
        "                if len(history) > self.n_step + 1:\n",
        "                    history.popleft()\n",
        "                while len(history) >= 1:\n",
        "                    yield tuple(history)\n",
        "                    history.popleft()\n",
        "                self.rewards.append(reward)\n",
        "                reward = 0.0\n",
        "                state = self.env.reset()\n",
        "                history.clear()\n",
        "    \n",
        "    def rewards_steps(self):\n",
        "        rewards_steps = self.rewards\n",
        "        self.rewards = []\n",
        "        return rewards_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68rUlP2Y9-H-"
      },
      "source": [
        "### Implementing Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2fvc_4Ltmkf"
      },
      "source": [
        "class ReplayMemory:\n",
        "    \n",
        "    def __init__(self, n_steps, capacity = 10000):\n",
        "        self.capacity = capacity\n",
        "        self.n_steps = n_steps\n",
        "        self.n_steps_iter = iter(n_steps)\n",
        "        self.buffer = deque()\n",
        "\n",
        "    def sample_batch(self, batch_size): # creates an iterator that returns random batches\n",
        "        ofs = 0\n",
        "        vals = list(self.buffer)\n",
        "        np.random.shuffle(vals)\n",
        "        while (ofs+1)*batch_size <= len(self.buffer):\n",
        "            yield vals[ofs*batch_size:(ofs+1)*batch_size]\n",
        "            ofs += 1\n",
        "\n",
        "    def run_steps(self, samples):\n",
        "        while samples > 0:\n",
        "            entry = next(self.n_steps_iter) # 10 consecutive steps\n",
        "            self.buffer.append(entry) # we put 200 for the current episode\n",
        "            samples -= 1\n",
        "        while len(self.buffer) > self.capacity: # we accumulate no more than the capacity (10000)\n",
        "            self.buffer.popleft()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPwjy3NWsnYa"
      },
      "source": [
        "## ------------------------- AI FOR DOOM (ai.py) -------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5zFNqHemeCK"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub4nIzcgy7mh"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yhJRW-3iYMX"
      },
      "source": [
        "### Importing the packages for OpenAI and Doom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnsThiOsiYSD"
      },
      "source": [
        "import gym\n",
        "import vizdoomgym\n",
        "from gym import wrappers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmojGP0H-BzI"
      },
      "source": [
        "## Part 1 - Building the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-ukjFhR-Gpv"
      },
      "source": [
        "### Making the Brain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNCe7smpy9-x"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, number_actions):\n",
        "        super(CNN, self).__init__()\n",
        "        self.convolution1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 5)\n",
        "        self.convolution2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3)\n",
        "        self.convolution3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 2)\n",
        "        self.fc1 = nn.Linear(in_features = self.count_neurons((1, 256, 256)), out_features = 40)\n",
        "        self.fc2 = nn.Linear(in_features = 40, out_features = number_actions)\n",
        "\n",
        "    def count_neurons(self, image_dim):\n",
        "        x = Variable(torch.rand(1, *image_dim))\n",
        "        x = F.relu(F.max_pool2d(self.convolution1(x), 3, 2))\n",
        "        x = F.relu(F.max_pool2d(self.convolution2(x), 3, 2))\n",
        "        x = F.relu(F.max_pool2d(self.convolution3(x), 3, 2))\n",
        "        return x.data.view(1, -1).size(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.convolution1(x), 3, 2))\n",
        "        x = F.relu(F.max_pool2d(self.convolution2(x), 3, 2))\n",
        "        x = F.relu(F.max_pool2d(self.convolution3(x), 3, 2))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tcc4Rma-IlR"
      },
      "source": [
        "### Making the Body"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnuK2xfRz4zf"
      },
      "source": [
        "class SoftmaxBody(nn.Module):\n",
        "    \n",
        "    def __init__(self, T):\n",
        "        super(SoftmaxBody, self).__init__()\n",
        "        self.T = T\n",
        "\n",
        "    def forward(self, outputs, number_actions=1):\n",
        "        probs = F.softmax(outputs * self.T)  \n",
        "        actions = probs.multinomial(num_samples=number_actions)\n",
        "        return actions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw-X1XWt-Ky1"
      },
      "source": [
        "### Making the AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfnhPGSAz48B"
      },
      "source": [
        "class AI:\n",
        "\n",
        "    def __init__(self, brain, body):\n",
        "        self.brain = brain\n",
        "        self.body = body\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        input = Variable(torch.from_numpy(np.array(inputs, dtype = np.float32)))\n",
        "        output = self.brain(input)\n",
        "        actions = self.body(output)\n",
        "        return actions.data.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYd3od1emzwo"
      },
      "source": [
        "## Part 2 - Training the AI with Deep Convolutional Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eql5YrXU-NX-"
      },
      "source": [
        "### Getting the Doom environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqfmQqbE2MFG"
      },
      "source": [
        "doom_env = PreprocessImage(gym.make('VizdoomCorridor-v0'), width = 256, height = 256, grayscale = True)\n",
        "doom_env = wrappers.Monitor(doom_env, \"videos\", force = True)\n",
        "number_actions = doom_env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13klJXK1nRwC"
      },
      "source": [
        "### Building an AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OX44kYzwCxU"
      },
      "source": [
        "cnn = CNN(number_actions)\n",
        "softmax_body = SoftmaxBody(T = 1.0)\n",
        "ai = AI(brain = cnn, body = softmax_body)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOtPlK1YnZUp"
      },
      "source": [
        "### Setting up Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyDvfKLxnZaG"
      },
      "source": [
        "n_steps = NStepProgress(env = doom_env, ai = ai, n_step = 10)\n",
        "memory = ReplayMemory(n_steps = n_steps, capacity = 10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kydH6Nengnu"
      },
      "source": [
        "### Implementing Eligibility Trace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hMk8IUBwIMi"
      },
      "source": [
        "def eligibility_trace(batch):\n",
        "    gamma = 0.99\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for series in batch:\n",
        "        input = Variable(torch.from_numpy(np.array([series[0].state, series[-1].state], dtype = np.float32)))\n",
        "        output = cnn(input)\n",
        "        cumul_reward = 0.0 if series[-1].done else output[1].data.max()\n",
        "        for step in reversed(series[:-1]):\n",
        "            cumul_reward = step.reward + gamma * cumul_reward\n",
        "        state = series[0].state\n",
        "        target = output[0].data\n",
        "        target[series[0].action] = cumul_reward\n",
        "        inputs.append(state)\n",
        "        targets.append(target)\n",
        "    return torch.from_numpy(np.array(inputs, dtype = np.float32)), torch.stack(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtpYvtmfnugD"
      },
      "source": [
        "### Making the moving average on 100 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R_P7WqswRB0"
      },
      "source": [
        "class MA:\n",
        "    def __init__(self, size):\n",
        "        self.list_of_rewards = []\n",
        "        self.size = size\n",
        "    def add(self, rewards):\n",
        "        if isinstance(rewards, list):\n",
        "            self.list_of_rewards += rewards\n",
        "        else:\n",
        "            self.list_of_rewards.append(rewards)\n",
        "        while len(self.list_of_rewards) > self.size:\n",
        "            del self.list_of_rewards[0]\n",
        "    def average(self):\n",
        "        return np.mean(self.list_of_rewards)\n",
        "ma = MA(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilnk4zdR-T73"
      },
      "source": [
        "### Training the AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXtPVCUGwSfY"
      },
      "source": [
        "loss = nn.MSELoss()\n",
        "optimizer = optim.Adam(cnn.parameters(), lr = 0.001)\n",
        "nb_epochs = 20\n",
        "for epoch in range(1, nb_epochs + 1):\n",
        "    memory.run_steps(samples=200)\n",
        "    for batch in memory.sample_batch(128):\n",
        "        inputs, targets = eligibility_trace(batch)\n",
        "        inputs, targets = Variable(inputs), Variable(targets)\n",
        "        predictions = cnn(inputs)\n",
        "        loss_error = loss(predictions, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss_error.backward()\n",
        "        optimizer.step()\n",
        "    rewards_steps = n_steps.rewards_steps()\n",
        "    ma.add(rewards_steps)\n",
        "    avg_reward = ma.average()\n",
        "    print(\"Epoch: %s, Average Reward: %s\" % (str(epoch), str(avg_reward)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le1t1oG5-WCW"
      },
      "source": [
        "## ----- EXTRA CODE ONLY SPECIFIC TO THIS COLAB NOTEBOOK - VISUALIZING THE AI IN ACTION -----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AT2264rZHvd"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eCUhUyIZGmY"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import gym\n",
        "import vizdoomgym\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from gym.wrappers import Monitor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiFw9mX8ZUIy"
      },
      "source": [
        "### Printing the input shape and number of possible actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvyGlfUUZcFV"
      },
      "source": [
        "env = gym.make('VizdoomCorridor-v0')\n",
        "action_num = env.action_space.n\n",
        "print(\"Number of possible actions: \", action_num)\n",
        "state = env.reset()\n",
        "state, reward, done, info = env.step(env.action_space.sample())\n",
        "print(state.shape)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7JgjufrZy6S"
      },
      "source": [
        "### Displaying a frame of the environment just to see how it is like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOqMJj76ZxpO"
      },
      "source": [
        "observation = env.reset()\n",
        "plt.imshow(observation)\n",
        "plt.show()\n",
        "print(observation.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cdZuHmZ-ZrC"
      },
      "source": [
        "### Making a helper function for the visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlOoqIqywW3I"
      },
      "source": [
        "def wrap_env(env):\n",
        "  env = Monitor(env, './vid',video_callable=lambda episode_id:True, force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvu5TKfH-c6-"
      },
      "source": [
        "### Running the AI on one episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEj72bI05NeH"
      },
      "source": [
        "gym_env = PreprocessImage(gym.make('VizdoomCorridor-v0'), width = 256, height = 256, grayscale = True)\n",
        "gym_env = wrappers.Monitor(gym_env, \"videos\", force = True)\n",
        "environment = wrap_env(gym_env)\n",
        "done = False\n",
        "observation = environment.reset()\n",
        "new_observation = observation\n",
        "actions_counter = Counter()\n",
        "prev_input = None\n",
        "img_array=[]\n",
        "while True:\n",
        "    # Feeding the game screen and getting the Q values for each action\n",
        "    actions = cnn(Variable(Variable(torch.from_numpy(observation.reshape(1, 1, 256, 256)))))\n",
        "    # Getting the action\n",
        "    action = np.argmax(actions.detach().numpy(), axis=-1)\n",
        "    actions_counter[str(action)] += 1 \n",
        "    # Selecting the action using epsilon greedy policy\n",
        "    environment.render()\n",
        "    observation = new_observation        \n",
        "    # Now performing the action and moving to the next state, next_obs, receive reward\n",
        "    new_observation, reward, done, _ = environment.step(action)\n",
        "    img_array.append(new_observation)\n",
        "    if done: \n",
        "        # observation = env.reset()\n",
        "        break\n",
        "environment.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p75VoM6Inlo7"
      },
      "source": [
        "### Outputting the video of the gameplay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHj2NJl4nqSj"
      },
      "source": [
        "folder_name = \"gameplay_frames\"\n",
        "video_name = \"agent_gameplay.avi\"\n",
        "\n",
        "if not os.path.exists(folder_name):\n",
        "    os.makedirs(folder_name)\n",
        "\n",
        "for i in range(len(img_array)):\n",
        "    plt.imsave(\"{}/{}_frame.jpg\".format(folder_name, i), img_array[i].reshape(256, 256))\n",
        "\n",
        "files = glob.glob(os.path.expanduser(\"{}/*\".format(folder_name)))\n",
        "frames_array = []\n",
        "\n",
        "for filename in sorted(files, key=lambda t: os.stat(t).st_mtime):\n",
        "    img = cv2.imread(filename)\n",
        "    height, width, layers = img.shape\n",
        "    size = (width,height)\n",
        "    frames_array.append(img)\n",
        "\n",
        "out = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
        "\n",
        "for i in range(len(frames_array)):\n",
        "    out.write(frames_array[i])\n",
        "\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}